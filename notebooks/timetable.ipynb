{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'wow', 'spark.executor.memory': '2g', 'spark.executor.instances': '64'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8387</td><td>application_1589299642358_2919</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2919/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2919_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8390</td><td>application_1589299642358_2922</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2922/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2922_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8393</td><td>application_1589299642358_2925</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2925/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2925_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8397</td><td>application_1589299642358_2929</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2929/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2929_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8398</td><td>application_1589299642358_2930</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2930/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2930_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8400</td><td>application_1589299642358_2932</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2932/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2932_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8401</td><td>application_1589299642358_2933</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2933/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2933_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8403</td><td>application_1589299642358_2935</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2935/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2935_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8405</td><td>application_1589299642358_2937</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2937/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2937_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8407</td><td>application_1589299642358_2939</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2939/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2939_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8408</td><td>application_1589299642358_2940</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2940/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2940_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8409</td><td>application_1589299642358_2941</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2941/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2941_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8410</td><td>application_1589299642358_2942</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2942/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2942_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8412</td><td>application_1589299642358_2944</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2944/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2944_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8414</td><td>application_1589299642358_2946</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2946/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2946_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8415</td><td>application_1589299642358_2947</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2947/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2947_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8417</td><td>application_1589299642358_2949</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2949/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2949_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8418</td><td>application_1589299642358_2950</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2950/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2950_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8419</td><td>application_1589299642358_2951</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2951/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2951_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8420</td><td>application_1589299642358_2952</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2952/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2952_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.app.name\": \"wow\",\n",
    "        \"spark.driver.memory\": '8g',\n",
    "        'spark.driver.maxResultSize': '6g',\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.instances\": \"64\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8421</td><td>application_1589299642358_2953</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2953/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2953_01_000001/ebouille\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fa6ab43e290>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'wow', 'spark.executor.memory': '2g', 'spark.executor.instances': '64'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8387</td><td>application_1589299642358_2919</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2919/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2919_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8390</td><td>application_1589299642358_2922</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2922/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2922_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8393</td><td>application_1589299642358_2925</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2925/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2925_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8397</td><td>application_1589299642358_2929</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2929/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2929_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8398</td><td>application_1589299642358_2930</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2930/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2930_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8400</td><td>application_1589299642358_2932</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2932/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2932_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8401</td><td>application_1589299642358_2933</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2933/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2933_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8403</td><td>application_1589299642358_2935</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2935/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2935_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8405</td><td>application_1589299642358_2937</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2937/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2937_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8407</td><td>application_1589299642358_2939</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2939/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2939_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8408</td><td>application_1589299642358_2940</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2940/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2940_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8409</td><td>application_1589299642358_2941</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2941/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2941_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8410</td><td>application_1589299642358_2942</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2942/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2942_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8412</td><td>application_1589299642358_2944</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2944/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2944_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8414</td><td>application_1589299642358_2946</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2946/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2946_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8415</td><td>application_1589299642358_2947</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2947/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2947_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8417</td><td>application_1589299642358_2949</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2949/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2949_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8418</td><td>application_1589299642358_2950</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2950/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2950_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8419</td><td>application_1589299642358_2951</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2951/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2951_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8420</td><td>application_1589299642358_2952</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2952/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2952_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8421</td><td>application_1589299642358_2953</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2953/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2953_01_000001/ebouille\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trips = spark.read.orc(\"/data/sbb/timetables/orc/trips/\")\n",
    "calendar = spark.read.orc(\"/data/sbb/timetables/orc/calendar/\")\n",
    "routes = spark.read.orc(\"/data/sbb/timetables/orc/routes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "|   route_id|service_id|             trip_id|     trip_headsign|trip_short_name|direction_id|\n",
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "|1-1-C-j19-1|  TA+b0001|5.TA.1-1-C-j19-1.3.R|Zofingen, Altachen|            108|           1|\n",
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "trips.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+---------+--------+------+--------+------+\n",
      "|service_id|monday|tuesday|wednesday|thursday|friday|saturday|sunday|\n",
      "+----------+------+-------+---------+--------+------+--------+------+\n",
      "|  TA+b0nx9|  true|   true|     true|    true|  true|   false| false|\n",
      "+----------+------+-------+---------+--------+------+--------+------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "calendar.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------------+---------------+----------+----------+\n",
      "|   route_id|agency_id|route_short_name|route_long_name|route_desc|route_type|\n",
      "+-----------+---------+----------------+---------------+----------+----------+\n",
      "|11-40-j19-1|      801|             040|               |       Bus|       700|\n",
      "+-----------+---------+----------------+---------------+----------+----------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "routes.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â select all services that are scheduled on mondays\n",
    "monday_service_id = (\n",
    "    calendar\n",
    "    .where(calendar.monday == True)\n",
    "    .select('service_id')\n",
    "    .dropDuplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â keep only trips that are scheduled on mondays\n",
    "monday_trips = (\n",
    "    trips\n",
    "    .join(f.broadcast(monday_service_id), 'service_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â add route information to trips\n",
    "route_trips = (\n",
    "    trips\n",
    "    .join(routes, 'route_id')\n",
    "    \n",
    "    #Â drop all transports that are not of type:\n",
    "    #Â InterCity, InterRegio, Regionalzug, S-Bahn, Bus, Tram\n",
    "    .where(routes.route_type.isin(['102', '103', '106', '400', '700', '900']))\n",
    "    \n",
    "    # create id that is used to merge with ist data\n",
    "    .withColumn(\n",
    "        'id',\n",
    "        f.when(routes.route_type.isin(['700', '900']),\n",
    "            f.concat_ws(':', f.lit('85'), routes.agency_id.cast('int').cast('string'), routes.route_short_name.cast('int').cast('string'))\n",
    "        ).otherwise(\n",
    "            f.concat_ws(':', f.lit('85'), routes.agency_id.cast('int').cast('string'), trips.trip_short_name.cast('int').cast('string'))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "route_trips.write.format('orc').save('/user/datavirus/route_trips.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "route_trips = spark.read.orc('/user/datavirus/route_trips.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â add delay probabilities to each route\n",
    "#Â route_probability = (\n",
    "#Â     route_trips\n",
    "#Â     .join(probability, 'id')\n",
    "#Â )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â route_probability.write.format('orc').save('/user/datavirus/route_probability.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â route_probability = spark.read.orc('/user/datavirus/route_probability.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â route_probability.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â read timetable stop times and filter values we are interested in\n",
    "\n",
    "stop_time = spark.read.orc(\"/data/sbb/timetables/orc/stop_times/\")\n",
    "\n",
    "stop_time = (\n",
    "    stop_time\n",
    "    .select(['trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence'])\n",
    "    .filter(\n",
    "        (stop_time.departure_time >= '05:00:00')\n",
    "        & (stop_time.departure_time <= '21:00:00')\n",
    "        & (stop_time.arrival_time >= '05:00:00')\n",
    "        & (stop_time.arrival_time <= '21:00:00') \n",
    "    )\n",
    "    .withColumn('station_id', stop_time.stop_id.substr(0, 7))\n",
    "    .drop('stop_id')\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load stations around zurich\n",
    "\n",
    "stations = spark.read.csv(\"../data/zurich_stations_ids.csv\")\n",
    "stations = (\n",
    "    stations\n",
    "    .select(stations._c0.alias('station_id'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove all stops that are not close to zurich\n",
    "\n",
    "stop_filtered = (\n",
    "    stop_time\n",
    "    .join(f.broadcast(stations), 'station_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# because we removed some stops there are now trips with a single stop\n",
    "# we only want to keep trips with multiple stops\n",
    "\n",
    "stop_keep = (\n",
    "    stop_filtered\n",
    "    .groupBy('trip_id')\n",
    "    .count()\n",
    "    .filter('count > 1')\n",
    "    .select('trip_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# because we removed stops there are now gaps in the stop_sequence\n",
    "# we recreate the stop_sequence with a rank function\n",
    "\n",
    "trip_window = Window.partitionBy('trip_id').orderBy(f.asc('stop_sequence'))\n",
    "stop_sequence = f.rank().over(trip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Â reset the stop_sequence\n",
    "\n",
    "stop_resequenced = (\n",
    "    stop_filtered\n",
    "    .join(stop_keep, 'trip_id')\n",
    "    .withColumn('stop_sequence', stop_sequence)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_resequenced.write.format('orc').save('/user/datavirus/stop_resequenced.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_resequenced = spark.read.orc('/user/datavirus/stop_resequenced.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we want to join the resequenced dataframe\n",
    "#Â with itself but with the stop_sequence\n",
    "# of the second dataframe shifted by one\n",
    "#Â for this we first have to create the\n",
    "# two dataframes\n",
    "\n",
    "departure = spark.read.orc('/user/datavirus/stop_resequenced.orc').alias('departure').repartition(100, 'trip_id')\n",
    "arrival = spark.read.orc('/user/datavirus/stop_resequenced.orc').alias('arrival').repartition(100, 'trip_id')\n",
    "arrival = (\n",
    "    arrival\n",
    "    .withColumn('stop_sequence', arrival.stop_sequence - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# join the dataframes, this gives us all connections\n",
    "\n",
    "raw_connections = (\n",
    "    departure\n",
    "    .join(arrival, ['trip_id', 'stop_sequence'])\n",
    "    .select(\n",
    "        departure.stop_sequence.alias('stop_sequence'),\n",
    "        departure.trip_id.alias('trip_id'),\n",
    "        departure.station_id.alias('start_id'),\n",
    "        departure.departure_time.alias('start_time'),\n",
    "        arrival.arrival_time.alias('stop_time'),\n",
    "        arrival.station_id.alias('stop_id')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o122.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:180)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.spark.SparkException: There is no enough memory to build hash map\n",
      "\tat org.apache.spark.sql.execution.joins.UnsafeHashedRelation$.apply(HashedRelation.scala:319)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:113)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:886)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:874)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:86)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdata/sdd/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2953/container_e06_1589299642358_2953_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 705, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/hdata/sdd/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2953/container_e06_1589299642358_2953_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hdata/sdd/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2953/container_e06_1589299642358_2953_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/hdata/sdd/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2953/container_e06_1589299642358_2953_01_000001/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o122.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:180)\n",
      "\t... 30 more\n",
      "Caused by: org.apache.spark.SparkException: There is no enough memory to build hash map\n",
      "\tat org.apache.spark.sql.execution.joins.UnsafeHashedRelation$.apply(HashedRelation.scala:319)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:113)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:886)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:874)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:86)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_connections.write.format('orc').save('/user/datavirus/raw_connections_test2.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 8421 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Warning: Master yarn-cluster is deprecated since 2.0. Please use master \"yarn\" with specified deploy mode instead.\n",
      "20/05/24 11:06:39 INFO RMProxy: Connecting to ResourceManager at iccluster044.iccluster.epfl.ch/10.90.38.20:8050\n",
      "20/05/24 11:06:39 INFO Client: Requesting a new application from cluster with 8 NodeManagers\n",
      "20/05/24 11:06:39 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.0.0.0-1634/0/resource-types.xml\n",
      "20/05/24 11:06:39 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (186368 MB per container)\n",
      "20/05/24 11:06:39 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "20/05/24 11:06:39 INFO Client: Setting up container launch context for our AM\n",
      "20/05/24 11:06:39 INFO Client: Setting up the launch environment for our AM container\n",
      "20/05/24 11:06:39 INFO Client: Preparing resources for our AM container\n",
      "20/05/24 11:06:41 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://iccluster044.iccluster.epfl.ch:8020/hdp/apps/3.0.0.0-1634/spark2/spark2-hdp-yarn-archive.tar.gz\n",
      "20/05/24 11:06:41 INFO Client: Source and destination file systems are the same. Not copying hdfs://iccluster044.iccluster.epfl.ch:8020/hdp/apps/3.0.0.0-1634/spark2/spark2-hdp-yarn-archive.tar.gz\n",
      "20/05/24 11:06:41 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://iccluster044.iccluster.epfl.ch:8020/hdp/apps/3.0.0.0-1634/spark2/spark2-hdp-hive-archive.tar.gz\n",
      "20/05/24 11:06:41 INFO Client: Source and destination file systems are the same. Not copying hdfs://iccluster044.iccluster.epfl.ch:8020/hdp/apps/3.0.0.0-1634/spark2/spark2-hdp-hive-archive.tar.gz\n",
      "20/05/24 11:06:41 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/netty-all-4.0.37.Final.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/netty-all-4.0.37.Final.jar\n",
      "20/05/24 11:06:41 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-rsc-0.5.0.3.0.0.0-1634.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/livy-rsc-0.5.0.3.0.0.0-1634.jar\n",
      "20/05/24 11:06:41 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-api-0.5.0.3.0.0.0-1634.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/livy-api-0.5.0.3.0.0.0-1634.jar\n",
      "20/05/24 11:06:42 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-core_2.11-0.5.0.3.0.0.0-1634.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/livy-core_2.11-0.5.0.3.0.0.0-1634.jar\n",
      "20/05/24 11:06:42 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-repl_2.11-0.5.0.3.0.0.0-1634.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/livy-repl_2.11-0.5.0.3.0.0.0-1634.jar\n",
      "20/05/24 11:06:42 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/commons-codec-1.9.jar\n",
      "20/05/24 11:06:42 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-3.2.6.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/datanucleus-api-jdo-3.2.6.jar\n",
      "20/05/24 11:06:42 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-core-3.2.10.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/datanucleus-core-3.2.10.jar\n",
      "20/05/24 11:06:43 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-3.2.9.jar -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/datanucleus-rdbms-3.2.9.jar\n",
      "20/05/24 11:06:43 INFO Client: Uploading resource file:/etc/spark2/3.0.0.0-1634/0/hive-site.xml -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/hive-site.xml\n",
      "20/05/24 11:06:43 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/R/lib/sparkr.zip#sparkr -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/sparkr.zip\n",
      "20/05/24 11:06:44 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/pyspark.zip\n",
      "20/05/24 11:06:44 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/py4j-0.10.7-src.zip\n",
      "20/05/24 11:06:44 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/05/24 11:06:44 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/05/24 11:06:44 INFO Client: Uploading resource file:/tmp/spark-114e76ca-f6aa-4954-afcc-9f4da373dee5/__spark_conf__1970181322456837027.zip -> hdfs://iccluster044.iccluster.epfl.ch:8020/user/ebouille/.sparkStaging/application_1589299642358_2953/__spark_conf__.zip\n",
      "20/05/24 11:06:44 INFO SecurityManager: Changing view acls to: ebouille\n",
      "20/05/24 11:06:44 INFO SecurityManager: Changing modify acls to: ebouille\n",
      "20/05/24 11:06:44 INFO SecurityManager: Changing view acls groups to: \n",
      "20/05/24 11:06:44 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/05/24 11:06:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ebouille); groups with view permissions: Set(); users  with modify permissions: Set(ebouille); groups with modify permissions: Set()\n",
      "20/05/24 11:06:44 INFO Client: Submitting application application_1589299642358_2953 to ResourceManager\n",
      "20/05/24 11:06:45 INFO YarnClientImpl: Submitted application application_1589299642358_2953\n",
      "20/05/24 11:06:45 INFO Client: Application report for application_1589299642358_2953 (state: ACCEPTED)\n",
      "20/05/24 11:06:45 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Sun May 24 11:06:45 +0200 2020] Application is Activated, waiting for resources to be assigned for AM.  Last Node which was processed for the application : iccluster065.iccluster.epfl.ch:45454 ( Partition : [], Total resource : <memory:186368, vCores:38>, Available resource : <memory:73728, vCores:7> ). Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:1490944, vCores:304> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 54.395603 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:1490944, vCores:304> ; Queue's used capacity (absolute resource) = <memory:811008, vCores:253> ; Queue's max capacity (absolute resource) = <memory:1490944, vCores:304> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1590311204999\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2953/\n",
      "\t user: ebouille\n",
      "20/05/24 11:06:45 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/05/24 11:06:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b0279d1-4f16-440f-a6ad-1470c87a81aa\n",
      "20/05/24 11:06:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-114e76ca-f6aa-4954-afcc-9f4da373dee5\n",
      "\n",
      "YARN Diagnostics: \n",
      "Exception was thrown 1 time(s) from Reporter thread.\n"
     ]
    }
   ],
   "source": [
    "raw_connections = spark.read.orc('/user/datavirus/raw_connections_test2.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_connections.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_connections = (\n",
    "    raw_connections\n",
    "    .join(route_trips, 'trip_id')\n",
    "    .withColumn('arrival_time_minute', raw_connections.stop_time.substr(0, 5))\n",
    "    .withColumn('arrival_time_hour', raw_connections.stop_time.substr(0, 2))\n",
    "    .withColumn('station_id', raw_connections.stop_id)\n",
    "    .withColumn(\n",
    "        'produkt_id', \n",
    "        f.when(\n",
    "            route_trips.route_type == '700',\n",
    "            f.lit('bus')            \n",
    "        ).otherwise(\n",
    "            f.when(\n",
    "                route_trips.route_type == '900',\n",
    "                f.lit('tram')\n",
    "            ).otherwise(f.lit('zug'))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_connections.write.format('orc').save('/user/datavirus/id_connections_new.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_connections = spark.read.orc('/user/datavirus/id_connections_new.orc').repartition(150, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663611"
     ]
    }
   ],
   "source": [
    "id_connections.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability = spark.read.orc('/user/datavirus/probability.orc').repartition(150, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we add the probability information to all connections\n",
    "\n",
    "probability_connections = (\n",
    "    id_connections\n",
    "    .join(probability, ['id', 'station_id', 'arrival_time_minute'], how='left_outer')\n",
    "    .select(\n",
    "        id_connections.stop_sequence,\n",
    "        id_connections.route_type,\n",
    "        id_connections.arrival_time_hour,\n",
    "        id_connections.produkt_id,\n",
    "        id_connections.start_id,\n",
    "        id_connections.start_time,\n",
    "        id_connections.trip_id,\n",
    "        probability.transport_type,\n",
    "        probability.line_text,\n",
    "        id_connections.stop_time,\n",
    "        id_connections.stop_id,\n",
    "        probability.delay_probability,\n",
    "        probability.delay_parameter\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability_connections.write.format('orc').save('/user/datavirus/probability_connections_new.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability_connections = spark.read.orc('/user/datavirus/probability_connections_new.orc').alias('probability_connections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for some connections we have no line specific probabilities \n",
    "# so we have to add transport probabilities as backup\n",
    "\n",
    "transport_probability = spark.read.orc('/user/datavirus/transport_probability.orc').alias('transport_probability')\n",
    "transport_probability = (\n",
    "    transport_probability\n",
    "    .withColumn('arrival_time_hour', transport_probability.ankunftszeit_hour)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we get the final dataframe\n",
    "\n",
    "connections = (\n",
    "    probability_connections\n",
    "    .join(transport_probability, ['produkt_id', 'arrival_time_hour'])\n",
    "    .select(\n",
    "        probability_connections.stop_sequence,\n",
    "        probability_connections.route_type,\n",
    "        probability_connections.start_id,\n",
    "        probability_connections.start_time,\n",
    "        probability_connections.trip_id,\n",
    "        probability_connections.produkt_id.alias('transport_type'),\n",
    "        probability_connections.line_text,\n",
    "        probability_connections.stop_time,\n",
    "        probability_connections.stop_id,\n",
    "        f.when(\n",
    "            f.col('probability_connections.delay_probability').isNotNull(),\n",
    "            f.col('probability_connections.delay_probability')\n",
    "        ).otherwise(\n",
    "            f.col('transport_probability.transport_delay_probability')\n",
    "        ).alias('delay_probability'),\n",
    "        f.when(f.col('probability_connections.delay_parameter').isNotNull(),\n",
    "              f.col('probability_connections.delay_parameter')\n",
    "        ).otherwise(\n",
    "            f.col('transport_probability.transport_delay_parameter')\n",
    "        ).alias('delay_parameter')\n",
    "    )\n",
    "    .orderBy([probability_connections.stop_time.desc(), probability_connections.stop_sequence.desc()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "connections.write.format('orc').save('/user/datavirus/connections_new.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "connections = spark.read.orc('/user/datavirus/connections_new.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "connections.write.format('csv').save('/user/datavirus/connections.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "connections = spark.read.csv('/user/datavirus/connections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+-----------------------+---+----+--------+-------+------------------+--------------------+\n",
      "|_c0|_c1|_c2    |_c3     |_c4                    |_c5|_c6 |_c7     |_c8    |_c9               |_c10                |\n",
      "+---+---+-------+--------+-----------------------+---+----+--------+-------+------------------+--------------------+\n",
      "|2  |700|8502209|10:05:00|9.TA.30-170-Y-j19-1.1.H|bus|null|10:05:00|8502209|0.9193866847990115|0.011589134124321832|\n",
      "|2  |700|8502771|10:05:00|392.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|493.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|360.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|345.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|439.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|224.TA.26-235-j19-1.4.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|390.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|410.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2  |700|8502771|10:05:00|450.TA.26-235-j19-1.5.R|bus|235 |10:05:00|8502771|0.9159136766383016|0.013732196842403623|\n",
      "+---+---+-------+--------+-----------------------+---+----+--------+-------+------------------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "connections.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "connections.write.csv('/user/datavirus/connections_test.csv', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = spark.read.csv('/user/datavirus/connections_test.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+----------+-----------------------+--------------+---------+---------+-------+------------------+--------------------+\n",
      "|stop_sequence|route_type|start_id|start_time|trip_id                |transport_type|line_text|stop_time|stop_id|delay_probability |delay_parameter     |\n",
      "+-------------+----------+--------+----------+-----------------------+--------------+---------+---------+-------+------------------+--------------------+\n",
      "|2            |700       |8502209 |10:05:00  |9.TA.30-170-Y-j19-1.1.H|bus           |null     |10:05:00 |8502209|0.9193866847990115|0.011589134124321832|\n",
      "|2            |700       |8502771 |10:05:00  |392.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |493.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |360.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |345.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |439.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |224.TA.26-235-j19-1.4.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |390.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |410.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |450.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "+-------------+----------+--------+----------+-----------------------+--------------+---------+---------+-------+------------------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "c.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
