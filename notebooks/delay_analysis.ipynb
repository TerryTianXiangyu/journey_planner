{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7676</td><td>application_1589299642358_2172</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2172/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2172_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A session has already been started. If you intend to recreate the session with new configurations, please include the -f argument.\n"
     ]
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.app.name\": \"datavirus_final\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f7540b67110>"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7611</td><td>application_1589299642358_2106</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2106/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2106_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7617</td><td>application_1589299642358_2112</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2112/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2112_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7624</td><td>application_1589299642358_2119</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2119/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2119_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7632</td><td>application_1589299642358_2126</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2126/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2126_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7633</td><td>application_1589299642358_2127</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2127/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2127_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7635</td><td>application_1589299642358_2129</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2129/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2129_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7640</td><td>application_1589299642358_2135</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2135/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2135_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7642</td><td>application_1589299642358_2138</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2138/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2138_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7644</td><td>application_1589299642358_2140</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2140/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2140_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7646</td><td>application_1589299642358_2142</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/cluster/app/application_1589299642358_2142\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster054.iccluster.epfl.ch:8188/applicationhistory/logs/iccluster069.iccluster.epfl.ch:45454/container_e06_1589299642358_2142_01_000001/container_e06_1589299642358_2142_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7651</td><td>application_1589299642358_2147</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2147/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2147_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7653</td><td>application_1589299642358_2149</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2149/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2149_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7656</td><td>application_1589299642358_2152</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2152/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2152_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7659</td><td>application_1589299642358_2155</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2155/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2155_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7664</td><td>application_1589299642358_2160</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2160/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2160_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7665</td><td>application_1589299642358_2161</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2161/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2161_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7667</td><td>application_1589299642358_2163</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2163/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2163_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7668</td><td>application_1589299642358_2164</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2164/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2164_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7670</td><td>application_1589299642358_2166</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2166/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2166_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7672</td><td>application_1589299642358_2168</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/cluster/app/application_1589299642358_2168\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster054.iccluster.epfl.ch:8188/applicationhistory/logs/iccluster065.iccluster.epfl.ch:45454/container_e06_1589299642358_2168_01_000001/container_e06_1589299642358_2168_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7674</td><td>application_1589299642358_2170</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2170/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2170_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7675</td><td>application_1589299642358_2171</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2171/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2171_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>7676</td><td>application_1589299642358_2172</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2172/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2172_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb = spark.read.orc('/data/sbb/orc/istdaten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+-----------------+------------+-----------+------------------+----------------+-----------+------------------+-------------+\n",
      "|betriebstag|   fahrt_bezeichner|betreiber_id|betreiber_abk|betreiber_name|produkt_id|linien_id|linien_text|umlauf_id|verkehrsmittel_text|zusatzfahrt_tf|faellt_aus_tf|  bpuic|haltestellen_name|ankunftszeit|an_prognose|an_prognose_status|    abfahrtszeit|ab_prognose|ab_prognose_status|durchfahrt_tf|\n",
      "+-----------+-------------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+-----------------+------------+-----------+------------------+----------------+-----------+------------------+-------------+\n",
      "| 03.09.2018|80:06____:17010:000|   80:06____|           DB|   DB Regio AG|       Zug|    17010|         RE|         |                 RE|         false|        false|8500090|     Basel Bad Bf|            |           |          PROGNOSE|03.09.2018 05:45|           |         UNBEKANNT|        false|\n",
      "+-----------+-------------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+-----------------+------------+-----------+------------------+----------------+-----------+------------------+-------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "sbb.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb = (\n",
    "    sbb\n",
    "    \n",
    "    # filter out any trips that we are not interested in\n",
    "    .where(sbb.zusatzfahrt_tf == 'false')\n",
    "    .where(sbb.faellt_aus_tf == 'false')\n",
    "    .where(sbb.durchfahrt_tf == 'false')\n",
    "    .where(sbb.ankunftszeit.isNotNull())\n",
    "    .where(sbb.an_prognose.isNotNull())\n",
    "    .where(sbb.an_prognose_status.isin('REAL', 'PROGNOSE'))\n",
    "    \n",
    "    # calculate delay\n",
    "    .withColumn('ankunftszeit_ts', f.unix_timestamp(f.col('ankunftszeit'), \"dd.MM.yyyy HH:mm\").cast('long'))\n",
    "    .withColumn('an_prognose_ts', f.unix_timestamp(f.col('an_prognose'), \"dd.MM.yyyy HH:mm:ss\").cast('long'))\n",
    "    .withColumn('delay', f.col('an_prognose_ts') - f.col('ankunftszeit_ts'))\n",
    "    .where(f.col('delay').isNotNull())\n",
    "    \n",
    "    # sometimes there are more multiple REAL and PROGNOSE values for the exactly same trip on the same day\n",
    "    # and time. There is no way which one is the closest to the actual arrival time of the train so we \n",
    "    # just drop all duplicates and keep one \"randomly\".\n",
    "    .select(['betriebstag', 'fahrt_bezeichner', 'produkt_id', 'linien_id', 'linien_text', 'bpuic', 'ankunftszeit', 'an_prognose_status', 'delay'])\n",
    "    .dropDuplicates(['betriebstag', 'fahrt_bezeichner', 'bpuic', 'ankunftszeit', 'an_prognose_status'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stations = spark.read.csv(\"../data/zurich_stations_ids.csv\")\n",
    "stations = (\n",
    "    stations\n",
    "    .select(stations._c0.alias('stop_id'))\n",
    ")\n",
    "stop_ids = [row['stop_id'] for row in stations.collect()]\n",
    "stop_ids = spark.sparkContext.broadcast(stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb_zurich = sbb.where(sbb.bpuic.isin(stop_ids.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delays = (\n",
    "    sbb_zurich\n",
    "    \n",
    "    # for each trip we only kept one REAL and one PROGNOSE row\n",
    "    # now we aggregate them into the same row for each trip\n",
    "    .groupBy(['ankunftszeit', 'fahrt_bezeichner', 'bpuic'])\n",
    "    .agg(\n",
    "        f.first(sbb_zurich.produkt_id).alias('produkt_id'),\n",
    "        f.first(sbb_zurich.linien_id).alias('linien_id'),\n",
    "        f.first(sbb_zurich.linien_text).alias('linien_text'),\n",
    "        f.first(f.when(sbb_zurich.an_prognose_status == 'REAL', sbb_zurich.delay).otherwise(None)).alias('real_delay'),\n",
    "        f.first(f.when(sbb_zurich.an_prognose_status == 'PROGNOSE', sbb_zurich.delay).otherwise(None)).alias('prognose_delay'),\n",
    "    )\n",
    "    \n",
    "    # if there is REAL delay we select it, otherwise we take the PROGNOSE delay \n",
    "    .withColumn('delay', f.when(f.col('real_delay').isNotNull(), f.col('real_delay')).otherwise(f.col('prognose_delay')))\n",
    "    .withColumn('is_delayed', f.when(f.col('delay') > 0, 1).otherwise(0))\n",
    "    \n",
    "    # create extra columns that contain time without date\n",
    "    .withColumn('ankunftszeit_minute', sbb_zurich.ankunftszeit.substr(12, 5))\n",
    "    .withColumn('ankunftszeit_hour', sbb_zurich.ankunftszeit.substr(12, 2))\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trip_probability = (\n",
    "    delays\n",
    "    \n",
    "    # group identical trips over all days\n",
    "    .groupBy(['fahrt_bezeichner', 'bpuic', 'ankunftszeit_minute'])\n",
    "    \n",
    "    # calculate delay probability and 1/(mean delay) for each trip\n",
    "    .agg(\n",
    "        f.first(delays.produkt_id).alias('produkt_id'),\n",
    "        f.first(delays.linien_id).alias('linien_id'),\n",
    "        f.first(delays.linien_text).alias('linien_text'),\n",
    "        f.mean(delays.is_delayed).alias('delay_probability'),\n",
    "        (1.0 / f.mean(f.when(delays.is_delayed == 1, f.when(delays.delay > 30*60, 30*60).otherwise(delays.delay)))).alias('delay_parameter'),\n",
    "        f.count(delays.delay).alias('n')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o4918.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 157 (cache at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 32 \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) \tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) \tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) \tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) \tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) \tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) \tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) \tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) \n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1361)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2172/container_e06_1589299642358_2172_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 705, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2172/container_e06_1589299642358_2172_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2172/container_e06_1589299642358_2172_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2172/container_e06_1589299642358_2172_01_000001/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o4918.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 157 (cache at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 32 \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) \tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) \tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) \tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) \tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) \tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) \tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) \tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) \n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1361)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_probability.write.format(\"orc\").save(\"/user/gflueck/trip_probability.orc\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "produkt_probability = (\n",
    "    delays\n",
    "    .groupBy(['ankunftszeit_hour', 'produkt_id'])\n",
    "    .agg(\n",
    "        f.mean(delays.is_delayed).alias('delay_probability'),\n",
    "        (1.0 / f.mean(f.when(delays.is_delayed == 1, f.when(delays.delay > 30*60, 30*60).otherwise(delays.delay)))).alias('delay_parameter'),\n",
    "        f.count(delays.delay).alias('n')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "produkt_probability.write.format(\"orc\").save(\"/user/gflueck/produkt_probability.orc\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line_probability = (\n",
    "    delays\n",
    "    .groupBy(['ankunftszeit_hour', 'linien_id'])\n",
    "    .agg(\n",
    "        f.mean(delays.is_delayed).alias('delay_probability'),\n",
    "        (1.0 / f.mean(f.when(delays.is_delayed == 1, f.when(delays.delay > 30*60, 30*60).otherwise(delays.delay)))).alias('delay_parameter'),\n",
    "        f.count(delays.delay).alias('n')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line_probability.write.format('orc').save('/user/gflueck/line_probability.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "u'Unable to infer schema for ORC. It must be specified manually.;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2172/container_e06_1589299642358_2172_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 476, in orc\n",
      "    return self._df(self._jreader.orc(_to_seq(self._spark._sc, path)))\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2172/container_e06_1589299642358_2172_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_2172/container_e06_1589299642358_2172_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "AnalysisException: u'Unable to infer schema for ORC. It must be specified manually.;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_probability = spark.read.orc(\"/user/gflueck/trip_probability.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "produkt_probability = spark.read.orc(\"/user/gflueck/produkt_probability.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line_probability = spark.read.orc('/user/gflueck/line_probability.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7102976948916947"
     ]
    }
   ],
   "source": [
    "line_probability.where(line_probability.n > 1000).count() * 1.0 / line_probability.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8633"
     ]
    }
   ],
   "source": [
    "line_probability.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_probability = (\n",
    "    trip_probability\n",
    "    .withColumn('ankunftszeit_hour', trip_probability.ankunftszeit_minute.substr(0, 3))\n",
    "    .join(f.broadcast(line_probability), ['linien_id', 'ankunftszeit_hour'])\n",
    "    .join(f.broadcast(produkt_probability), ['produkt_id', 'ankunftszeit_hour'])\n",
    "    .select(\n",
    "        trip_probability.ankunftszeit_minute.alias('arrival_time_minute'),\n",
    "        trip_probability.fahrt_bezeichner.alias('trip_id'),\n",
    "        trip_probability.bpuic.alias('station_id'),\n",
    "        trip_probability.produkt_id.alias('transport_type'),\n",
    "        trip_probability.linien_id.alias('line_id'),\n",
    "        trip_probability.linien_text.alias('line_text'),\n",
    "        trip_probability.delay_probability.alias('trip_delay_probability'),\n",
    "        trip_probability.delay_parameter.alias('trip_delay_parameter'),\n",
    "        trip_probability.n.alias('trip_n'),\n",
    "        line_probability.delay_probability.alias('line_delay_probability'),\n",
    "        line_probability.delay_parameter.alias('line_delay_parameter'),\n",
    "        line_probability.n.alias('line_n'),\n",
    "        produkt_probability.delay_probability.alias('transport_delay_probability'),\n",
    "        produkt_probability.delay_parameter.alias('transport_delay_parameter'),\n",
    "        produkt_probability.n.alias('transport_n')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_probability.write.format('orc').save('/user/gflueck/full_probability.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability = (\n",
    "    full_probability\n",
    "    .select(\n",
    "        full_probability.arrival_time_minute,\n",
    "        full_probability.trip_id,\n",
    "        full_probability.station_id,\n",
    "        full_probability.transport_type,\n",
    "        full_probability.line_id,\n",
    "        full_probability.line_text,\n",
    "        (\n",
    "            f.when(full_probability.trip_n > 500, full_probability.trip_delay_probability)\n",
    "            .otherwise(\n",
    "                f.when(full_probability.line_n > 500, full_probability.line_delay_probability)\n",
    "                .otherwise(full_probability.transport_delay_probability)\n",
    "            )\n",
    "        ).alias('delay_probability'),\n",
    "        (\n",
    "            f.when(full_probability.trip_n > 500, full_probability.trip_delay_parameter)\n",
    "            .otherwise(\n",
    "                f.when(full_probability.line_n > 500, full_probability.line_delay_parameter)\n",
    "                .otherwise(full_probability.transport_delay_parameter)\n",
    "            )\n",
    "        ).alias('delay_parameter')  \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability.write.format('orc').save('/user/gflueck/probability.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+----------+--------------+-------+---------+------------------+--------------------+\n",
      "|arrival_time_minute|trip_id        |station_id|transport_type|line_id|line_text|delay_probability |delay_parameter     |\n",
      "+-------------------+---------------+----------+--------------+-------+---------+------------------+--------------------+\n",
      "| 21:42             |85:11:10:002   |8503202   |Zug           |10     |EC       |0.8202508288487688|0.014650408388357685|\n",
      "| 02:58             |85:11:13714:001|8503000   |Zug           |13714  |SN1      |0.6519774011299435|0.006361772034664491|\n",
      "| 02:12             |85:11:13783:001|8503000   |Zug           |13783  |SN8      |0.8833798882681564|0.007784998646087192|\n",
      "| 10:08             |85:11:14037:001|8503006   |Zug           |14037  |S24      |0.8132095748704679|0.015289947865916156|\n",
      "| 13:56             |85:11:14051:002|8503010   |Zug           |14051  |S24      |0.81328451151898  |0.015783488520259153|\n",
      "| 15:14             |85:11:14055:002|8503204   |Zug           |14055  |S24      |0.8229764472710196|0.014770271373540382|\n",
      "| 17:21             |85:11:14066:002|8503202   |Zug           |14066  |S24      |0.8888496639022103|0.009861344482875898|\n",
      "| 19:57             |85:11:14077:001|8503307   |Zug           |14077  |S24      |0.8456830210103476|0.012050985796682373|\n",
      "| 20:51             |85:11:14080:001|8503202   |Zug           |14080  |S24      |0.8206369829555474|0.01426967463547332 |\n",
      "| 22:21             |85:11:14084:001|8503006   |Zug           |14084  |S24      |0.8255917107270856|0.013662843813113116|\n",
      "+-------------------+---------------+----------+--------------+-------+---------+------------------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "probability.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
