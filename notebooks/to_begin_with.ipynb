{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Begin With...\n",
    "\n",
    "### Name your spark application as `GASPAR_final` or `GROUP_NAME_final`.\n",
    "\n",
    "<div class='alert alert-info'><b>Any application without a proper name would be promptly killed.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'datavirus_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5840</td><td>application_1589299642358_0329</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0329/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0329_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5845</td><td>application_1589299642358_0334</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0334/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0334_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5848</td><td>application_1589299642358_0337</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0337/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0337_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5850</td><td>application_1589299642358_0339</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0339/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0339_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5851</td><td>application_1589299642358_0340</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0340/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0340_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5854</td><td>application_1589299642358_0343</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0343/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0343_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5856</td><td>application_1589299642358_0345</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0345/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0345_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5857</td><td>application_1589299642358_0346</td><td>pyspark</td><td>shutting_down</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0346/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0346_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5858</td><td>application_1589299642358_0347</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0347/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0347_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5860</td><td>application_1589299642358_0349</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0349/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0349_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5863</td><td>application_1589299642358_0352</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0352/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0352_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5865</td><td>application_1589299642358_0354</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0354/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0354_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"datavirus_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5866</td><td>application_1589299642358_0355</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0355/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0355_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f209b67c850>"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'datavirus_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5840</td><td>application_1589299642358_0329</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0329/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0329_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5845</td><td>application_1589299642358_0334</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0334/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0334_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5848</td><td>application_1589299642358_0337</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0337/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0337_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5850</td><td>application_1589299642358_0339</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0339/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0339_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5851</td><td>application_1589299642358_0340</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0340/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0340_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5854</td><td>application_1589299642358_0343</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0343/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0343_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5856</td><td>application_1589299642358_0345</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0345/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0345_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5858</td><td>application_1589299642358_0347</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0347/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0347_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5860</td><td>application_1589299642358_0349</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0349/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0349_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5863</td><td>application_1589299642358_0352</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0352/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0352_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5865</td><td>application_1589299642358_0354</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0354/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0354_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>5866</td><td>application_1589299642358_0355</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_0355/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_0355_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the [SBB actual data](https://opentransportdata.swiss/en/dataset/istdaten) in ORC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb = spark.read.orc('/data/sbb/orc/istdaten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- betriebstag: string (nullable = true)\n",
      " |-- fahrt_bezeichner: string (nullable = true)\n",
      " |-- betreiber_id: string (nullable = true)\n",
      " |-- betreiber_abk: string (nullable = true)\n",
      " |-- betreiber_name: string (nullable = true)\n",
      " |-- produkt_id: string (nullable = true)\n",
      " |-- linien_id: string (nullable = true)\n",
      " |-- linien_text: string (nullable = true)\n",
      " |-- umlauf_id: string (nullable = true)\n",
      " |-- verkehrsmittel_text: string (nullable = true)\n",
      " |-- zusatzfahrt_tf: string (nullable = true)\n",
      " |-- faellt_aus_tf: string (nullable = true)\n",
      " |-- bpuic: string (nullable = true)\n",
      " |-- haltestellen_name: string (nullable = true)\n",
      " |-- ankunftszeit: string (nullable = true)\n",
      " |-- an_prognose: string (nullable = true)\n",
      " |-- an_prognose_status: string (nullable = true)\n",
      " |-- abfahrtszeit: string (nullable = true)\n",
      " |-- ab_prognose: string (nullable = true)\n",
      " |-- ab_prognose_status: string (nullable = true)\n",
      " |-- durchfahrt_tf: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "sbb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(betriebstag=u'03.09.2018', fahrt_bezeichner=u'80:06____:17010:000', betreiber_id=u'80:06____', betreiber_abk=u'DB', betreiber_name=u'DB Regio AG', produkt_id=u'Zug', linien_id=u'17010', linien_text=u'RE', umlauf_id=u'', verkehrsmittel_text=u'RE', zusatzfahrt_tf=u'false', faellt_aus_tf=u'false', bpuic=u'8500090', haltestellen_name=u'Basel Bad Bf', ankunftszeit=u'', an_prognose=u'', an_prognose_status=u'PROGNOSE', abfahrtszeit=u'03.09.2018 05:45', ab_prognose=u'', ab_prognose_status=u'UNBEKANNT', durchfahrt_tf=u'false'), Row(betriebstag=u'03.09.2018', fahrt_bezeichner=u'80:06____:17012:000', betreiber_id=u'80:06____', betreiber_abk=u'DB', betreiber_name=u'DB Regio AG', produkt_id=u'Zug', linien_id=u'17012', linien_text=u'RE', umlauf_id=u'', verkehrsmittel_text=u'RE', zusatzfahrt_tf=u'false', faellt_aus_tf=u'false', bpuic=u'8500090', haltestellen_name=u'Basel Bad Bf', ankunftszeit=u'', an_prognose=u'', an_prognose_status=u'PROGNOSE', abfahrtszeit=u'03.09.2018 06:34', ab_prognose=u'', ab_prognose_status=u'UNBEKANNT', durchfahrt_tf=u'false'), Row(betriebstag=u'03.09.2018', fahrt_bezeichner=u'80:06____:17013:000', betreiber_id=u'80:06____', betreiber_abk=u'DB', betreiber_name=u'DB Regio AG', produkt_id=u'Zug', linien_id=u'17013', linien_text=u'RE', umlauf_id=u'', verkehrsmittel_text=u'RE', zusatzfahrt_tf=u'false', faellt_aus_tf=u'false', bpuic=u'8500090', haltestellen_name=u'Basel Bad Bf', ankunftszeit=u'03.09.2018 06:25', an_prognose=u'', an_prognose_status=u'UNBEKANNT', abfahrtszeit=u'', ab_prognose=u'', ab_prognose_status=u'PROGNOSE', durchfahrt_tf=u'false'), Row(betriebstag=u'03.09.2018', fahrt_bezeichner=u'80:06____:17014:000', betreiber_id=u'80:06____', betreiber_abk=u'DB', betreiber_name=u'DB Regio AG', produkt_id=u'Zug', linien_id=u'17014', linien_text=u'RE', umlauf_id=u'', verkehrsmittel_text=u'RE', zusatzfahrt_tf=u'false', faellt_aus_tf=u'false', bpuic=u'8500090', haltestellen_name=u'Basel Bad Bf', ankunftszeit=u'', an_prognose=u'', an_prognose_status=u'PROGNOSE', abfahrtszeit=u'03.09.2018 09:48', ab_prognose=u'', ab_prognose_status=u'UNBEKANNT', durchfahrt_tf=u'false'), Row(betriebstag=u'03.09.2018', fahrt_bezeichner=u'80:06____:17015:000', betreiber_id=u'80:06____', betreiber_abk=u'DB', betreiber_name=u'DB Regio AG', produkt_id=u'Zug', linien_id=u'17015', linien_text=u'RE', umlauf_id=u'', verkehrsmittel_text=u'RE', zusatzfahrt_tf=u'false', faellt_aus_tf=u'false', bpuic=u'8500090', haltestellen_name=u'Basel Bad Bf', ankunftszeit=u'03.09.2018 08:06', an_prognose=u'', an_prognose_status=u'UNBEKANNT', abfahrtszeit=u'', ab_prognose=u'', ab_prognose_status=u'PROGNOSE', durchfahrt_tf=u'false')]"
     ]
    }
   ],
   "source": [
    "sbb.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the station list data [BFKOORD_GEO](https://opentransportdata.swiss/en/cookbook/hafas-rohdaten-format-hrdf/#Abgrenzung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = spark.read.csv('/data/sbb/stations/bfkoordgeo.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StationID: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      " |-- Remark: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "metadata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+------+----------------+\n",
      "|StationID|Longitude| Latitude|Height|          Remark|\n",
      "+---------+---------+---------+------+----------------+\n",
      "|  0000002|26.074412|44.446770|     0|       Bucuresti|\n",
      "|  0000003| 1.811446|50.901549|     0|          Calais|\n",
      "|  0000004| 1.075329|51.284212|     0|      Canterbury|\n",
      "|  0000005|-3.543547|50.729172|     0|          Exeter|\n",
      "|  0000007| 9.733756|46.922368|   744|Fideris, Bahnhof|\n",
      "+---------+---------+---------+------+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "metadata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382"
     ]
    }
   ],
   "source": [
    "df_stations = metadata.withColumn(\"dlon\", F.radians(F.col(\"Longitude\")) - math.radians(8.540192)) \\\n",
    "             .withColumn(\"dlat\", F.radians(F.col(\"Latitude\")) - math.radians(47.378177)) \\\n",
    "             .withColumn(\"Distance_from_Zurich\", F.asin(F.sqrt( F.sin(F.col(\"dlat\") / 2) ** 2 + math.cos(math.radians(47.378177))\n",
    "                                               *F.cos(F.radians(F.col(\"Latitude\"))) * F.sin(F.col(\"dlon\") / 2) ** 2)) * 2 * 3963 * 5280) \\\n",
    "             .drop(\"dlon\", \"dlat\") \\\n",
    "             .filter(F.col(\"Distance_from_Zurich\")<15000)\n",
    "\n",
    "zurich_stations = list(df_stations.select('Remark').toPandas()['Remark'])\n",
    "print(len(zurich_stations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+------+--------------------+--------------------+\n",
      "|StationID|Longitude| Latitude|Height|              Remark|Distance_from_Zurich|\n",
      "+---------+---------+---------+------+--------------------+--------------------+\n",
      "|  0000176| 8.521961|47.351679|     0|Zimmerberg-Basist...|  10676.361524930166|\n",
      "|  8502572| 8.513918|47.370293|   421|Zürich, Goldbrunn...|   7107.377290914878|\n",
      "|  8503000| 8.540192|47.378177|   408|           Zürich HB|2.323101710999253...|\n",
      "|  8503001| 8.488940|47.391481|   399|   Zürich Altstetten|  13572.481490959273|\n",
      "|  8503003| 8.548466|47.366611|   411|  Zürich Stadelhofen|   4693.551820280705|\n",
      "|  8503004| 8.561372|47.350124|   408|Zürich Tiefenbrunnen|   11506.98247693669|\n",
      "|  8503006| 8.544115|47.411529|   442|     Zürich Oerlikon|  12218.830294000149|\n",
      "|  8503007| 8.544636|47.418747|   442|      Zürich Seebach|  14856.992780652563|\n",
      "|  8503009| 8.533588|47.347440|   409|  Zürich Wollishofen|  11343.522830011038|\n",
      "|  8503010| 8.530805|47.364099|   409|         Zürich Enge|   5641.253510656322|\n",
      "|  8503011| 8.523462|47.371472|   405|     Zürich Wiedikon|   4807.886502430274|\n",
      "|  8503015| 8.529359|47.393032|   425|    Zürich Wipkingen|  6050.3533490488735|\n",
      "|  8503020| 8.517106|47.385195|   415|   Zürich Hardbrücke|   6257.735615033529|\n",
      "|  8503051| 8.518424|47.362632|   421|         Zürich Binz|   7824.134947499741|\n",
      "|  8503052| 8.507942|47.364781|   446|  Zürich Friesenberg|    9357.23915794205|\n",
      "|  8503053| 8.503047|47.364988|   459|   Zürich Schweighof|  10373.203289755538|\n",
      "|  8503054| 8.495197|47.364997|   485|      Zürich Triemli|  12125.004050604579|\n",
      "|  8503059| 8.548130|47.366353|   411|Zürich Stadelhofe...|   4743.532106869039|\n",
      "|  8503069| 8.583204|47.351016|   524|       Zürich Rehalp|   14546.26629164279|\n",
      "|  8503083| 8.560281|47.368089|   443|    Zürich, Römerhof|   6185.387416616962|\n",
      "+---------+---------+---------+------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_stations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+-------+--------+---------+\n",
      "|    id1|    lon1|     lat1|    id2|    lon2|     lat2|\n",
      "+-------+--------+---------+-------+--------+---------+\n",
      "|0000176|8.521961|47.351679|0000176|8.521961|47.351679|\n",
      "|0000176|8.521961|47.351679|8502572|8.513918|47.370293|\n",
      "|0000176|8.521961|47.351679|8503000|8.540192|47.378177|\n",
      "|0000176|8.521961|47.351679|8503001|8.488940|47.391481|\n",
      "|0000176|8.521961|47.351679|8503003|8.548466|47.366611|\n",
      "+-------+--------+---------+-------+--------+---------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "test = df_stations.select('StationID','Longitude','Latitude')\n",
    "joinedDF = test.crossJoin(test).toDF('id1','lon1','lat1','id2','lon2','lat2')\n",
    "joinedDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "distance = (\n",
    "    joinedDF\n",
    "        .withColumn(\"dlon\", F.radians(F.col(\"lon1\")) - F.radians(F.col(\"lon2\")))\n",
    "        .withColumn(\"dlat\", F.radians(F.col(\"lat1\")) - F.radians(F.col(\"lat2\"))) \n",
    "        .withColumn(\"Distance\", F.asin(F.sqrt( F.sin(F.col(\"dlat\") / 2) ** 2 + F.cos(F.radians(\"lat2\"))\n",
    "                                           *F.cos(F.radians(F.col(\"lat1\"))) * F.sin(F.col(\"dlon\") / 2) ** 2)) * 2 * 3963 * 5280) \\\n",
    "        .drop(\"dlon\", \"dlat\") \n",
    "        .filter(F.col(\"Distance\")<500)\n",
    "        .withColumn(\"Walking_time\",F.round(2+F.col(\"Distance\")/50).cast(IntegerType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+-------+--------+---------+------------------+------------+\n",
      "|    id1|    lon1|     lat1|    id2|    lon2|     lat2|          Distance|Walking_time|\n",
      "+-------+--------+---------+-------+--------+---------+------------------+------------+\n",
      "|0000176|8.521961|47.351679|0000176|8.521961|47.351679|               0.0|           2|\n",
      "|8502572|8.513918|47.370293|8502572|8.513918|47.370293|               0.0|           2|\n",
      "|8503000|8.540192|47.378177|8503000|8.540192|47.378177|               0.0|           2|\n",
      "|8503000|8.540192|47.378177|8503088|8.539170|47.377431|371.62272536447483|           9|\n",
      "|8503000|8.540192|47.378177|8503446|8.541715|47.378846|448.94025446812276|          11|\n",
      "|8503000|8.540192|47.378177|8587348|8.539338|47.377241| 401.8110520444312|          10|\n",
      "|8503000|8.540192|47.378177|8587349|8.541742|47.377560| 444.6416307076613|          11|\n",
      "|8503001|8.488940|47.391481|8503001|8.488940|47.391481|               0.0|           2|\n",
      "|8503001|8.488940|47.391481|8591056|8.488376|47.391109| 194.6818748525234|           6|\n",
      "|8503001|8.488940|47.391481|8591057|8.489905|47.392066|320.25886115557165|           8|\n",
      "|8503003|8.548466|47.366611|8503003|8.548466|47.366611|               0.0|           2|\n",
      "|8503003|8.548466|47.366611|8503059|8.548130|47.366353|125.63987893661184|           5|\n",
      "|8503003|8.548466|47.366611|8576195|8.547647|47.365414| 481.8092497282149|          12|\n",
      "|8503004|8.561372|47.350124|8503004|8.561372|47.350124|               0.0|           2|\n",
      "|8503004|8.561372|47.350124|8576182|8.560994|47.350416|141.84373133551784|           5|\n",
      "|8503006|8.544115|47.411529|8503006|8.544115|47.411529|               0.0|           2|\n",
      "|8503006|8.544115|47.411529|8580449|8.544790|47.411496|167.25675657987733|           5|\n",
      "|8503006|8.544115|47.411529|8591062|8.543934|47.412404| 322.6691343514852|           8|\n",
      "|8503007|8.544636|47.418747|8503007|8.544636|47.418747|               0.0|           2|\n",
      "|8503009|8.533588|47.347440|8503009|8.533588|47.347440|               0.0|           2|\n",
      "+-------+--------+---------+-------+--------+---------+------------------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "distance.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BETRIEBSTAG**: date of the trip\n",
    "\n",
    "**FAHRT_BEZEICHNER**: identifies the trip\n",
    "\n",
    "**BETREIBER_ABK, BETREIBER_NAME**: operator (name will contain the full name, e.g. Schweizerische Bundesbahnen for SBB)\n",
    "\n",
    "**PRODUKT_ID**: type of transport, e.g. train, bus\n",
    "\n",
    "**LINIEN_ID**: for trains, this is the train number\n",
    "\n",
    "**LINIEN_TEXT,VERKEHRSMITTEL_TEXT**: for trains, the service type (IC, IR, RE, etc.)\n",
    "\n",
    "**ZUSATZFAHRT_TF**: boolean, true if this is an additional trip (not part of the regular schedule)\n",
    "\n",
    "**FAELLT_AUS_TF**: boolean, true if this trip failed (cancelled or not completed)\n",
    "\n",
    "**HALTESTELLEN_NAME**: name of the stop\n",
    "\n",
    "**ANKUNFTSZEIT**: arrival time at the stop according to schedule\n",
    "\n",
    "**AN_PROGNOSE**: actual arrival time (when AN_PROGNOSE_STATUS is GESCHAETZT)\n",
    "\n",
    "**AN_PROGNOSE_STATUS**: look only at lines when this is GESCHAETZT. This indicates that AN_PROGNOSE is the measured time of arrival.\n",
    "\n",
    "**ABFAHRTSZEIT**: departure time at the stop according to schedule\n",
    "\n",
    "**AB_PROGNOSE**: actual departure time (when AN_PROGNOSE_STATUS is GESCHAETZT)\n",
    "\n",
    "**AB_PROGNOSE_STATUS**: look only at lines when this is GESCHAETZT. This indicates that AB_PROGNOSE is the measured time of arrival.\n",
    "\n",
    "**DURCHFAHRT_TF**: boolean, true if the transport does not stop there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_all =sbb.where(sbb[\"HALTESTELLEN_NAME\"].isin(zurich_stations))\n",
    "df_real = df_all.where(df_all['AN_PROGNOSE_STATUS']=='REAL')\n",
    "df_real = df_real.where(df_real['AB_PROGNOSE_STATUS']=='REAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cache a simpler dataframe with only relevant columns and without duplicated rows\n",
    "simple_df = (\n",
    "    df_real\n",
    "    .withColumn('arrival_time', F.when(F.col('ankunftszeit') == '', None).otherwise(F.col('ankunftszeit')))\n",
    "    .withColumn('arrival', F.unix_timestamp(F.col('ankunftszeit'), \"dd.mm.yyyy hh:mm\").cast('long'))\n",
    "    .withColumn('real_arrival', F.unix_timestamp(F.col('an_prognose'), \"dd.mm.yyyy hh:mm:ss\").cast(\"long\"))\n",
    "    .withColumn('arrival_delay',F.col('real_arrival')-F.col('arrival'))\n",
    "    .withColumn('departure_time', F.when(F.col('abfahrtszeit') == '', None).otherwise(F.col('abfahrtszeit'))) \n",
    "    .withColumn('departure', F.unix_timestamp(F.col('abfahrtszeit'), \"dd.mm.yyyy hh:mm\").cast('long'))\n",
    "    .withColumn('real_departure', F.unix_timestamp(F.col('ab_prognose'), \"dd.mm.yyyy hh:mm:ss\").cast(\"long\"))\n",
    "    .withColumn('departure_delay',F.col('real_departure')-F.col('departure'))\n",
    "    .select('arrival_time','departure_time','arrival_delay','departure_delay','BETRIEBSTAG','FAHRT_BEZEICHNER','LINIEN_ID','PRODUKT_ID','BPUIC','HALTESTELLEN_NAME')\n",
    "    .toDF('Arrival_Time','Departure_Time','Arrival_Delay','Departure_Delay','Day','Trip_ID','Line_ID', 'Type','Station_ID', 'Station_Name')\n",
    "    .dropDuplicates()\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Arrival_Time=u'30.12.2019 11:07', Departure_Time=u'30.12.2019 11:07', Arrival_Delay=-9, Departure_Delay=43, Day=u'30.12.2019', Trip_ID=u'85:11:18341:001', Line_ID=u'18341', Type=u'Zug', Station_ID=u'8503003', Station_Name=u'Z\\xfcrich Stadelhofen'), Row(Arrival_Time=u'30.12.2019 19:11', Departure_Time=u'30.12.2019 19:11', Arrival_Delay=None, Departure_Delay=None, Day=u'30.12.2019', Trip_ID=u'85:11:18572:001', Line_ID=u'18572', Type=u'Zug', Station_ID=u'8503020', Station_Name=u'Z\\xfcrich Hardbr\\xfccke'), Row(Arrival_Time=u'30.12.2019 10:22', Departure_Time=u'30.12.2019 10:22', Arrival_Delay=6, Departure_Delay=62, Day=u'30.12.2019', Trip_ID=u'85:11:18638:001', Line_ID=u'18638', Type=u'Zug', Station_ID=u'8503004', Station_Name=u'Z\\xfcrich Tiefenbrunnen'), Row(Arrival_Time=u'30.12.2019 06:59', Departure_Time=u'30.12.2019 07:00', Arrival_Delay=133, Departure_Delay=98, Day=u'30.12.2019', Trip_ID=u'85:11:18825:001', Line_ID=u'18825', Type=u'Zug', Station_ID=u'8503006', Station_Name=u'Z\\xfcrich Oerlikon'), Row(Arrival_Time=u'30.12.2019 13:49', Departure_Time=u'30.12.2019 13:49', Arrival_Delay=None, Departure_Delay=None, Day=u'30.12.2019', Trip_ID=u'85:11:18850:001', Line_ID=u'18850', Type=u'Zug', Station_ID=u'8503011', Station_Name=u'Z\\xfcrich Wiedikon')]"
     ]
    }
   ],
   "source": [
    "simple_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove rows correspoding to trips appearing only once on the dataframe\n",
    "# These removable trips actually come from / go to stations outside Zurich\n",
    "ids = simple_df.groupBy('Trip_ID','Day').count()\n",
    "ids = ids.where(ids['count']>1).select('Trip_ID').distinct()\n",
    "df = simple_df.join(ids, \"Trip_ID\").orderBy('Trip_ID','Arrival_Time','Departure_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solve null values:\n",
    "# Remove columns with more than two null-values\n",
    "# Copy \"Arrival_Time\" value if \"Departure_Time\" is null and viceversa\n",
    "df = df.dropna(thresh=1,subset=('Arrival_Time','Departure_Time')) \\\n",
    "        .withColumn(\"Departure\",F.coalesce(df.Departure_Time,df.Arrival_Time))\\\n",
    "        .withColumn(\"Arrival\", F.coalesce(df.Arrival_Time,df.Departure_Time))\\\n",
    "        .drop(\"Departure_Time\",\"Arrival_Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delays = df.groupBy('Trip_ID','Station_Name').agg(F.mean('Arrival_Delay'),F.mean('Departure_Delay'))\n",
    "df = df.join(delays,on =['Trip_ID','Station_Name']).drop('Arrival_Delay','Departure_Delay')\n",
    "df_day = df.where(df['Day']=='13.05.2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b4b32ced0e476b937da01a9041e1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank stops by departure_time for every trip and day \n",
    "from pyspark.sql import Window\n",
    "trip_window = Window.partitionBy('Trip_ID','Day').orderBy(F.asc('Departure'))\n",
    "trip_rank = F.rank().over(trip_window).alias('stop')\n",
    "begin = df_day.select('*', trip_rank).alias('begin').orderBy('Trip_ID','Arrival','Departure').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+----------+-------+----+----------+----------------+----------------+------------------+--------------------+----+\n",
      "|        Trip_ID|     Station_Name|       Day|Line_ID|Type|Station_ID|       Departure|         Arrival|avg(Arrival_Delay)|avg(Departure_Delay)|stop|\n",
      "+---------------+-----------------+----------+-------+----+----------+----------------+----------------+------------------+--------------------+----+\n",
      "|85:11:10706:001|  Zürich Oerlikon|23.03.2019|  10706| Zug|   8503006|23.03.2019 06:15|23.03.2019 06:13|              77.0|                50.0|   1|\n",
      "|85:11:10706:001|        Zürich HB|23.03.2019|  10706| Zug|   8503000|23.03.2019 06:32|23.03.2019 06:21|             -15.0|                64.5|   2|\n",
      "|85:11:10706:001|  Zürich Oerlikon|24.03.2019|  10706| Zug|   8503006|24.03.2019 06:15|24.03.2019 06:13|              77.0|                50.0|   1|\n",
      "|85:11:10706:001|        Zürich HB|24.03.2019|  10706| Zug|   8503000|24.03.2019 06:32|24.03.2019 06:21|             -15.0|                64.5|   2|\n",
      "|85:11:13710:001|Zürich Hardbrücke|01.01.2019|  13710| Zug|   8503020|01.01.2019 01:02|01.01.2019 01:02|             97.95|  161.85555555555555|   1|\n",
      "+---------------+-----------------+----------+-------+----+----------+----------------+----------------+------------------+--------------------+----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "begin.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataframe for every connection \n",
    "end = begin.drop('Departure','avg(Departure_Delay)').withColumn('stop', begin.stop -1).alias('end')\n",
    "data = begin.drop('Arrival','avg(Arrival_Delay)').join(end, on=['stop','Day','Trip_ID','Type','Line_ID'])\\\n",
    "            .drop('stop').orderBy('Trip_ID','Arrival','Departure')\\\n",
    "            .toDF('Day','Trip_ID','Line_ID','Type','Start_ID','Start_Station','Start_Time','Stop_ID', 'Stop_Station','Stop_Time','Start_Delay','Stop_Delay')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o684.collectToPython.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:85)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:206)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.SparkException: There is no enough memory to build hash map\n",
      "\tat org.apache.spark.sql.execution.joins.UnsafeHashedRelation$.apply(HashedRelation.scala:319)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:113)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:886)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:874)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:86)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_0355/container_e06_1589299642358_0355_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 504, in take\n",
      "    return self.limit(num).collect()\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_0355/container_e06_1589299642358_0355_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 466, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_0355/container_e06_1589299642358_0355_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_0355/container_e06_1589299642358_0355_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/hdata/sde/hadoop/yarn/local/usercache/ebouille/appcache/application_1589299642358_0355/container_e06_1589299642358_0355_01_000001/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o684.collectToPython.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:85)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:206)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.SparkException: There is no enough memory to build hash map\n",
      "\tat org.apache.spark.sql.execution.joins.UnsafeHashedRelation$.apply(HashedRelation.scala:319)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:113)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:886)\n",
      "\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:874)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:86)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.format(\"csv\").save(\"../data/complete_sensor_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------+----+--------+------------------+----------------+-------+---------------+----------------+\n",
      "|       Day|        Trip_ID|Line_ID|Type|Start_ID|     Start_Station|      Start_Time|Stop_ID|   Stop_Station|       Stop_Time|\n",
      "+----------+---------------+-------+----+--------+------------------+----------------+-------+---------------+----------------+\n",
      "|02.12.2019|85:11:10040:001|  10040| Zug| 8503006|   Zürich Oerlikon|02.12.2019 22:15|8503000|      Zürich HB|02.12.2019 22:21|\n",
      "|02.12.2019|85:11:10049:001|  10049| Zug| 8503000|         Zürich HB|02.12.2019 22:39|8503006|Zürich Oerlikon|02.12.2019 22:45|\n",
      "|02.12.2019|85:11:14016:004|  14016| Zug| 8503009|Zürich Wollishofen|02.12.2019 05:02|8503010|    Zürich Enge|02.12.2019 05:05|\n",
      "|02.12.2019|85:11:14016:004|  14016| Zug| 8503010|       Zürich Enge|02.12.2019 05:06|8503011|Zürich Wiedikon|02.12.2019 05:07|\n",
      "|02.12.2019|85:11:14016:004|  14016| Zug| 8503011|   Zürich Wiedikon|02.12.2019 05:07|8503000|      Zürich HB|02.12.2019 05:12|\n",
      "|02.12.2019|85:11:14016:004|  14016| Zug| 8503000|         Zürich HB|02.12.2019 05:14|8503006|Zürich Oerlikon|02.12.2019 05:18|\n",
      "|02.12.2019|85:11:14017:003|  14017| Zug| 8503000|         Zürich HB|02.12.2019 05:12|8503006|Zürich Oerlikon|02.12.2019 05:16|\n",
      "|02.12.2019|85:11:14082:006|  14082| Zug| 8503009|Zürich Wollishofen|02.12.2019 21:43|8503010|    Zürich Enge|02.12.2019 21:47|\n",
      "|02.12.2019|85:11:14082:006|  14082| Zug| 8503010|       Zürich Enge|02.12.2019 21:48|8503011|Zürich Wiedikon|02.12.2019 21:49|\n",
      "|02.12.2019|85:11:14082:006|  14082| Zug| 8503011|   Zürich Wiedikon|02.12.2019 21:49|8503000|      Zürich HB|02.12.2019 21:53|\n",
      "+----------+---------------+-------+----+--------+------------------+----------------+-------+---------------+----------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "data.where(data['Day']=='02.12.2019').where(data['Type']=='Zug').show(10) #.where(data['Trip_ID']=='85:11:14082:006').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+--------+--------------------+----------------+-------+--------------------+----------------+\n",
      "|       Day|   Line_ID|Type|Start_ID|       Start_Station|      Start_Time|Stop_ID|        Stop_Station|       Stop_Time|\n",
      "+----------+----------+----+--------+--------------------+----------------+-------+--------------------+----------------+\n",
      "|02.12.2019|85:773:787| Bus| 8591347|Zürich, Schürgist...|02.12.2019 20:48|8591047|    Zürich, Aubrücke|02.12.2019 20:49|\n",
      "|02.12.2019|85:773:787| Bus| 8591047|    Zürich, Aubrücke|02.12.2019 20:49|8591225|Zürich, Genossens...|02.12.2019 20:52|\n",
      "|02.12.2019|85:773:787| Bus| 8591225|Zürich, Genossens...|02.12.2019 20:52|8591318|    Zürich, Riedbach|02.12.2019 20:53|\n",
      "|02.12.2019|85:773:787| Bus| 8591318|    Zürich, Riedbach|02.12.2019 20:53|8591172|   Zürich, Hagenholz|02.12.2019 20:54|\n",
      "|02.12.2019|85:773:787| Bus| 8591172|   Zürich, Hagenholz|02.12.2019 20:54|8591256|Zürich, Leutschen...|02.12.2019 20:55|\n",
      "|02.12.2019|85:773:787| Bus| 8591256|Zürich, Leutschen...|02.12.2019 21:03|8591172|   Zürich, Hagenholz|02.12.2019 21:04|\n",
      "|02.12.2019|85:773:787| Bus| 8591172|   Zürich, Hagenholz|02.12.2019 21:04|8591318|    Zürich, Riedbach|02.12.2019 21:05|\n",
      "|02.12.2019|85:773:787| Bus| 8591318|    Zürich, Riedbach|02.12.2019 21:05|8591225|Zürich, Genossens...|02.12.2019 21:06|\n",
      "|02.12.2019|85:773:787| Bus| 8591225|Zürich, Genossens...|02.12.2019 21:06|8591047|    Zürich, Aubrücke|02.12.2019 21:09|\n",
      "|02.12.2019|85:773:787| Bus| 8591047|    Zürich, Aubrücke|02.12.2019 21:09|8591347|Zürich, Schürgist...|02.12.2019 21:10|\n",
      "+----------+----------+----+--------+--------------------+----------------+-------+--------------------+----------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "data.where(data['Day']=='02.12.2019').where(data['Type']=='Bus').drop('Trip_ID').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----+--------+--------------------+----------------+-------+--------------------+----------------+\n",
      "|       Day|    Line_ID|Type|Start_ID|       Start_Station|      Start_Time|Stop_ID|        Stop_Station|       Stop_Time|\n",
      "+----------+-----------+----+--------+--------------------+----------------+-------+--------------------+----------------+\n",
      "|02.12.2019|85:3849:015|Tram| 8591101|Zürich, Bucheggplatz|02.12.2019 20:07|8591246|    Zürich, Laubiweg|02.12.2019 20:08|\n",
      "|02.12.2019|85:3849:015|Tram| 8591246|    Zürich, Laubiweg|02.12.2019 20:09|8591335|Zürich, Schaffhau...|02.12.2019 20:10|\n",
      "|02.12.2019|85:3849:015|Tram| 8591335|Zürich, Schaffhau...|02.12.2019 20:10|8591324|Zürich, Röslistrasse|02.12.2019 20:11|\n",
      "|02.12.2019|85:3849:015|Tram| 8591324|Zürich, Röslistrasse|02.12.2019 20:11|8591298|Zürich, Ottikerst...|02.12.2019 20:12|\n",
      "|02.12.2019|85:3849:015|Tram| 8591298|Zürich, Ottikerst...|02.12.2019 20:12|8591373|Zürich, Sonneggst...|02.12.2019 20:13|\n",
      "|02.12.2019|85:3849:015|Tram| 8591373|Zürich, Sonneggst...|02.12.2019 20:13|8591174|   Zürich, Haldenegg|02.12.2019 20:14|\n",
      "|02.12.2019|85:3849:015|Tram| 8591174|   Zürich, Haldenegg|02.12.2019 20:14|8588078|     Zürich, Central|02.12.2019 20:16|\n",
      "|02.12.2019|85:3849:015|Tram| 8588078|     Zürich, Central|02.12.2019 20:16|8591327|Zürich, Rudolf-Br...|02.12.2019 20:17|\n",
      "|02.12.2019|85:3849:015|Tram| 8591327|Zürich, Rudolf-Br...|02.12.2019 20:17|8591309|     Zürich, Rathaus|02.12.2019 20:18|\n",
      "|02.12.2019|85:3849:015|Tram| 8591309|     Zürich, Rathaus|02.12.2019 20:18|8591183|    Zürich, Helmhaus|02.12.2019 20:19|\n",
      "+----------+-----------+----+--------+--------------------+----------------+-------+--------------------+----------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "data.where(data['Day']=='02.12.2019').where(data['Type']=='Tram').drop('Trip_ID').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
