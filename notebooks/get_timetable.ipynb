{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get \"ideal\" connections timetable using files under HDFS\n",
    "\n",
    "In this notebook, we select the most relevant trips for our project and convert them to connections between stops so that we have a transportation schedule for an \"ideal\" working day. We also add important information to the connections so that the mapping with the historical SBB data it is easier.  The notebook is structured as follows: \n",
    "\n",
    "*   **[Start Spark](#spark)** \n",
    "*   **[Get relevant trips](#relevant)**  \n",
    "*   **[Transform trips to connections](#raw_connections)** \n",
    "*   **[Add useful mappingg information](#id_connections)**  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'spark'></a>\n",
    "### 1. Start Spark\n",
    "\n",
    "\n",
    "We will be using a Spark Session for performing didfferent transformations and actions on the raw dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.app.name\": \"wow\",\n",
    "        \"spark.driver.memory\": '8g',\n",
    "        'spark.driver.maxResultSize': '6g',\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.instances\": \"64\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8421</td><td>application_1589299642358_2953</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2953/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2953_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fa6ab43e290>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'relevant'></a>\n",
    "### 2. Get relevant trips\n",
    "\n",
    "We load and check the data from the files <code>trips.txt</code> and <code>routes.txt</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "|   route_id|service_id|             trip_id|     trip_headsign|trip_short_name|direction_id|\n",
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "|1-1-C-j19-1|  TA+b0001|5.TA.1-1-C-j19-1.3.R|Zofingen, Altachen|            108|           1|\n",
      "+-----------+----------+--------------------+------------------+---------------+------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "trips = spark.read.orc(\"/data/sbb/timetables/orc/trips/\")\n",
    "trips.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------------+---------------+----------+----------+\n",
      "|   route_id|agency_id|route_short_name|route_long_name|route_desc|route_type|\n",
      "+-----------+---------+----------------+---------------+----------+----------+\n",
      "|11-40-j19-1|      801|             040|               |       Bus|       700|\n",
      "+-----------+---------+----------------+---------------+----------+----------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "routes = spark.read.orc(\"/data/sbb/timetables/orc/routes/\")\n",
    "routes.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join both dataframes in order to obtain useful mapping information (used later) and we select only the relevant routes (where the transport type is InterCity, InterRegio, Regionalzug, S-Bahn, Bus, Tram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add route information to trips\n",
    "route_trips = (\n",
    "    trips\n",
    "    .join(routes, 'route_id')\n",
    "    \n",
    "    # drop all transports that are not of type:\n",
    "    # InterCity, InterRegio, Regionalzug, S-Bahn, Bus, Tram\n",
    "    .where(routes.route_type.isin(['102', '103', '106', '400', '700', '900']))\n",
    "    \n",
    "    # create id that is used to merge with ist data\n",
    "    .withColumn(\n",
    "        'id',\n",
    "        f.when(routes.route_type.isin(['700', '900']),\n",
    "            f.concat_ws(':', f.lit('85'), routes.agency_id.cast('int').cast('string'), routes.route_short_name.cast('int').cast('string'))\n",
    "        ).otherwise(\n",
    "            f.concat_ws(':', f.lit('85'), routes.agency_id.cast('int').cast('string'), trips.trip_short_name.cast('int').cast('string'))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "route_trips.write.format('orc').save('/user/datavirus/route_trips.orc', mode='overwrite')\n",
    "#route_trips = spark.read.orc('/user/datavirus/route_trips.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'raw_connections'></a>\n",
    "### 3. Transform trips to connections\n",
    "\n",
    "The file <code>stop_times.txt</code>  contains all the timing information for every stop and trip but we have to rehsape the dataframe in terms of connections (link between two sequenced stops). We will only use the trips ocurring between 5h and 21h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read timetable stop times and filter values we are interested in\n",
    "\n",
    "stop_time = spark.read.orc(\"/data/sbb/timetables/orc/stop_times/\")\n",
    "\n",
    "stop_time = (\n",
    "    stop_time\n",
    "    .select(['trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence'])\n",
    "    .filter(\n",
    "        (stop_time.departure_time >= '05:00:00')\n",
    "        & (stop_time.departure_time <= '21:00:00')\n",
    "        & (stop_time.arrival_time >= '05:00:00')\n",
    "        & (stop_time.arrival_time <= '21:00:00') \n",
    "    )\n",
    "    .withColumn('station_id', stop_time.stop_id.substr(0, 7))\n",
    "    .drop('stop_id')\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out all stations that are not within 15 km of Zurich and those that only appear once (meaning they go to/come from outside Zurich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load stations around zurich\n",
    "stations = spark.read.csv(\"../data/zurich_stations_ids.csv\")\n",
    "stations = (\n",
    "    stations\n",
    "    .select(stations._c0.alias('station_id'))\n",
    ")\n",
    "\n",
    "# remove all stops that are not close to zurich\n",
    "stop_filtered = (\n",
    "    stop_time\n",
    "    .join(f.broadcast(stations), 'station_id')\n",
    ")\n",
    "\n",
    "# because we removed some stops there are now trips with a single stop\n",
    "# we only want to keep trips with multiple stops\n",
    "\n",
    "stop_keep = (\n",
    "    stop_filtered\n",
    "    .groupBy('trip_id')\n",
    "    .count()\n",
    "    .filter('count > 1')\n",
    "    .select('trip_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transform trips (rows containing only one stop) to connections (link between two stops), we have to merge the dataframe with itself (but shifted of one stop) so that each arrival station matches with each departure stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# because we removed stops there are now gaps in the stop_sequence\n",
    "# we recreate the stop_sequence with a rank function\n",
    "trip_window = Window.partitionBy('trip_id').orderBy(f.asc('stop_sequence'))\n",
    "stop_sequence = f.rank().over(trip_window)\n",
    "\n",
    "stop_resequenced = (\n",
    "    stop_filtered\n",
    "    .join(stop_keep, 'trip_id')\n",
    "    .withColumn('stop_sequence', stop_sequence)\n",
    ")\n",
    "\n",
    "stop_resequenced.write.format('orc').save('/user/datavirus/stop_resequenced.orc', mode='overwrite')\n",
    "departure = spark.read.orc('/user/datavirus/stop_resequenced.orc').alias('departure').repartition(100, 'trip_id')\n",
    "arrival = spark.read.orc('/user/datavirus/stop_resequenced.orc').alias('arrival').repartition(100, 'trip_id')\n",
    "\n",
    "arrival = arrival.withColumn('stop_sequence', arrival.stop_sequence - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# join the dataframes, this gives us all connections\n",
    "\n",
    "raw_connections = (\n",
    "    departure\n",
    "    .join(arrival, ['trip_id', 'stop_sequence'])\n",
    "    .select(\n",
    "        departure.stop_sequence.alias('stop_sequence'),\n",
    "        departure.trip_id.alias('trip_id'),\n",
    "        departure.station_id.alias('start_id'),\n",
    "        departure.departure_time.alias('start_time'),\n",
    "        arrival.arrival_time.alias('stop_time'),\n",
    "        arrival.station_id.alias('stop_id')\n",
    "    )\n",
    ")\n",
    "\n",
    "raw_connections.write.format('orc').save('/user/datavirus/raw_connections_test2.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'id_connections'></a>\n",
    "### 4. Add useful mapping information\n",
    "\n",
    "Now that we have the timetable that we were looking for, we will complete it with relevant information about the trips and routes so that the mapping with the historical data can be done more easily. Doing a simple count(), we find that our network has more than 1 million and a half connections (edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_connections = (\n",
    "    raw_connections\n",
    "    .join(route_trips, 'trip_id')\n",
    "    .withColumn('arrival_time_minute', raw_connections.stop_time.substr(0, 5))\n",
    "    .withColumn('arrival_time_hour', raw_connections.stop_time.substr(0, 2))\n",
    "    .withColumn('station_id', raw_connections.stop_id)\n",
    "    .withColumn(\n",
    "        'produkt_id', \n",
    "        f.when(\n",
    "            route_trips.route_type == '700',\n",
    "            f.lit('bus')            \n",
    "        ).otherwise(\n",
    "            f.when(\n",
    "                route_trips.route_type == '900',\n",
    "                f.lit('tram')\n",
    "            ).otherwise(f.lit('zug'))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_connections.write.format('orc').save('/user/datavirus/id_connections_new.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663611"
     ]
    }
   ],
   "source": [
    "id_connections.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
